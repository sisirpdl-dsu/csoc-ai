{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7edec9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "{\n",
    "  \"cells\": [\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"id\": \"851d188e\",\n",
    "      \"metadata\": {\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\"\n",
    "        },\n",
    "        \"id\": \"851d188e\",\n",
    "        \"outputId\": \"c88db24d-fc96-4949-fd89-2331b2274863\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Torch 2.3.1+cu121 CUDA True\\n\",\n",
    "            \"Triton 2.3.0\\n\",\n",
    "            \"Versions 2.0.1 4.43.3 0.34.2 0.11.1 2.20.0\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"GPU_TRAINING=True\\n\",\n",
    "        \"FORCE_DISABLE_TRITON=False\\n\",\n",
    "        \"TORCH_VERSION='2.3.1'\\n\",\n",
    "        \"TRITON_VERSION='2.3.0'\\n\",\n",
    "        \"import os,importlib,sys,subprocess\\n\",\n",
    "        \"UNINSTALL_UNUSED=True\\n\",\n",
    "        \"# Remove prior torch to avoid mix\\n\",\n",
    "        \"subprocess.run([sys.executable,'-m','pip','uninstall','-y','torch','torchvision','torchaudio','torchtext','triton'],stdout=subprocess.DEVNULL,stderr=subprocess.DEVNULL)\\n\",\n",
    "        \"if UNINSTALL_UNUSED:\\n\",\n",
    "        \"    subprocess.run([sys.executable,'-m','pip','uninstall','-y','fastai','timm','sentence-transformers','bigframes','cudf-cu12','cuml-cu12','dask-cudf-cu12','torchtune','spacy','albumentations','albucore','xgboost'],stdout=subprocess.DEVNULL,stderr=subprocess.DEVNULL)\\n\",\n",
    "        \"subprocess.check_call([sys.executable,'-m','pip','install','-q','--upgrade','pip'])\\n\",\n",
    "        \"subprocess.check_call([sys.executable,'-m','pip','install','-q','numpy==2.0.1'])\\n\",\n",
    "        \"if GPU_TRAINING:\\n\",\n",
    "        \"    subprocess.check_call([sys.executable,'-m','pip','install','-q',f'torch=={TORCH_VERSION}','--index-url','https://download.pytorch.org/whl/cu121'])\\n\",\n",
    "        \"    if not FORCE_DISABLE_TRITON:\\n\",\n",
    "        \"        subprocess.check_call([sys.executable,'-m','pip','install','-q',f'triton=={TRITON_VERSION}'])\\n\",\n",
    "        \"else:\\n\",\n",
    "        \"    subprocess.check_call([sys.executable,'-m','pip','install','-q',f'torch=={TORCH_VERSION}+cpu','-f','https://download.pytorch.org/whl/torch_stable.html'])\\n\",\n",
    "        \"subprocess.check_call([sys.executable,'-m','pip','install','-q','fsspec==2024.5.0','gcsfs==2024.5.0','transformers==4.43.3','accelerate==0.34.2','datasets==2.20.0','trl==0.10.1','peft==0.11.1','bitsandbytes==0.44.0','safetensors==0.4.3','sentencepiece==0.2.0'])\\n\",\n",
    "        \"import torch\\n\",\n",
    "        \"print('Torch',torch.__version__,'CUDA',torch.cuda.is_available())\\n\",\n",
    "        \"has_triton=importlib.util.find_spec('triton') is not None\\n\",\n",
    "        \"if has_triton:\\n\",\n",
    "        \"    import triton; print('Triton',getattr(triton,'__version__','?'))\\n\",\n",
    "        \"if FORCE_DISABLE_TRITON:\\n\",\n",
    "        \"    os.environ['USE_TRITON']='0'; os.environ['TRITON_DISABLE']='1'\\n\",\n",
    "        \"USE_4BIT_RUNTIME_OK=True\\n\",\n",
    "        \"if GPU_TRAINING and torch.cuda.is_available():\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        import bitsandbytes as bnb; assert hasattr(bnb,'nn')\\n\",\n",
    "        \"    except Exception:\\n\",\n",
    "        \"        USE_4BIT_RUNTIME_OK=False\\n\",\n",
    "        \"else:\\n\",\n",
    "        \"    USE_4BIT_RUNTIME_OK=False\\n\",\n",
    "        \"os.environ['AUTO_DISABLE_4BIT']='1' if not USE_4BIT_RUNTIME_OK else '0'\\n\",\n",
    "        \"import numpy,transformers,accelerate,peft,datasets\\n\",\n",
    "        \"print('Versions',numpy.__version__,transformers.__version__,accelerate.__version__,peft.__version__,datasets.__version__)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"id\": \"828cca11\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"828cca11\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"BASE_MODEL='microsoft/Phi-3-mini-4k-instruct'\\n\",\n",
    "        \"OUTPUT_DIR='phi3_lora_adapter'\\n\",\n",
    "        \"MERGED_OUTPUT_DIR='phi3_merged'\\n\",\n",
    "        \"USE_4BIT=True\\n\",\n",
    "        \"LORA_R=32\\n\",\n",
    "        \"LORA_ALPHA=64\\n\",\n",
    "        \"LORA_DROPOUT=0.05\\n\",\n",
    "        \"NUM_EPOCHS=5\\n\",\n",
    "        \"BATCH_SIZE=4\\n\",\n",
    "        \"GR_ACCUM=4\\n\",\n",
    "        \"LEARNING_RATE=2e-4\\n\",\n",
    "        \"WARMUP_RATIO=0.05\\n\",\n",
    "        \"MAX_SEQ_LEN=1024\\n\",\n",
    "        \"SEED=42\\n\",\n",
    "        \"import torch,os,random\\n\",\n",
    "        \"if not torch.cuda.is_available():\\n\",\n",
    "        \"    USE_4BIT=False\\n\",\n",
    "        \"# Optional Hugging Face token (set env HF_TOKEN or assign directly below)\\n\",\n",
    "        \"HF_TOKEN=os.environ.get('HF_TOKEN') or None\\n\",\n",
    "        \"# Candidate public model IDs to try\\n\",\n",
    "        \"CANDIDATE_MODELS=[\\n\",\n",
    "        \"    BASE_MODEL\\n\",\n",
    "        \"]\\n\",\n",
    "        \"random.seed(SEED); torch.manual_seed(SEED)\\n\",\n",
    "        \"os.makedirs(OUTPUT_DIR,exist_ok=True)\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"id\": \"ba01a788\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"ba01a788\",\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\",\n",
    "          \"height\": 125\n",
    "        },\n",
    "        \"outputId\": \"984aa03f-5aa9-4427-8a74-8511bc275fd0\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"display_data\",\n",
    "          \"data\": {\n",
    "            \"text/plain\": [\n",
    "              \"<IPython.core.display.HTML object>\"\n",
    "            ],\n",
    "            \"text/html\": [\n",
    "              \"...\"\n",
    "            ]\n",
    "          },\n",
    "          \"metadata\": {}\n",
    "        },\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"Saving kql_eval.jsonl to kql_eval.jsonl\\n\",\n",
    "            \"Saving kql_train.jsonl to kql_train.jsonl\\n\",\n",
    "            \"Train records: 49  Eval records (external): 5\\n\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"train_records=[]\\n\",\n",
    "        \"eval_records=[]\\n\",\n",
    "        \"import os,sys,json\\n\",\n",
    "        \"DATA_PATHS=[]\\n\",\n",
    "        \"# Upload JSONL files. Any filename containing 'eval' or 'validation' (case-insensitive) treated as eval set.\\n\",\n",
    "        \"if 'google.colab' in sys.modules or 'COLAB_RELEASE_TAG' in os.environ:\\n\",\n",
    "        \"    from google.colab import files\\n\",\n",
    "        \"    uploaded=files.upload()\\n\",\n",
    "        \"    for name,data in uploaded.items():\\n\",\n",
    "        \"        with open(name,'wb') as f: f.write(data)\\n\",\n",
    "        \"        DATA_PATHS.append(name)\\n\",\n",
    "        \"else:\\n\",\n",
    "        \"    raise RuntimeError('Colab upload environment not detected.')\\n\",\n",
    "        \"if not DATA_PATHS:\\n\",\n",
    "        \"    raise ValueError('No files uploaded.')\\n\",\n",
    "        \"\\n\",\n",
    "        \"def load_jsonl(path):\\n\",\n",
    "        \"    out=[]\\n\",\n",
    "        \"    with open(path,'r',encoding='utf-8') as f:\\n\",\n",
    "        \"        for line in f:\\n\",\n",
    "        \"            line=line.strip()\\n\",\n",
    "        \"            if not line: continue\\n\",\n",
    "        \"            try:\\n\",\n",
    "        \"                obj=json.loads(line)\\n\",\n",
    "        \"            except Exception:\\n\",\n",
    "        \"                continue\\n\",\n",
    "        \"            instr=obj.get('instruction') or obj.get('prompt') or obj.get('input')\\n\",\n",
    "        \"            kql=obj.get('kql') or obj.get('output')\\n\",\n",
    "        \"            if instr and kql:\\n\",\n",
    "        \"                out.append({'instruction':instr,'kql':kql})\\n\",\n",
    "        \"    return out\\n\",\n",
    "        \"for p in DATA_PATHS:\\n\",\n",
    "        \"    if os.path.exists(p):\\n\",\n",
    "        \"        recs=load_jsonl(p)\\n\",\n",
    "        \"        lname=p.lower()\\n\",\n",
    "        \"        if ('eval' in lname) or ('validation' in lname):\\n\",\n",
    "        \"            eval_records.extend(recs)\\n\",\n",
    "        \"        else:\\n\",\n",
    "        \"            train_records.extend(recs)\\n\",\n",
    "        \"print(f'Train records: {len(train_records)}  Eval records (external): {len(eval_records)}')\\n\",\n",
    "        \"if not train_records:\\n\",\n",
    "        \"    raise ValueError('No valid training instruction/kql records found.')\\n\",\n",
    "        \"# If no external eval records, a split will be created later.\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"# !rm -r *\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"iQ0g0wwL9eBC\"\n",
    "      },\n",
    "      \"id\": \"iQ0g0wwL9eBC\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"id\": \"d2e44d53\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"d2e44d53\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"INSTRUCTION_PREFIX= \\\"\\\"\\\"You are an expert KQL assistant for Microsoft Sentinel. Your task is to convert a user's natural language request into a single, valid KQL query.\\n\",\n",
    "        \"\\n\",\n",
    "        \"Rules:\\n\",\n",
    "        \"1.  Output ONLY the raw KQL query. Do not include any commentary, explanations, or markdown backticks.\\n\",\n",
    "        \"2.  Always include a bounded time filter (e.g., `... | where TimeGenerated > ago(24h)`).\\n\",\n",
    "        \"3.  If the user provides a specific entity (like a username, IP, or filename), use it directly in the query.\\n\",\n",
    "        \"4.  If the user's request is generic (e.g., \\\"a user\\\" or \\\"an IP\\\"), use a realistic but clearly example entity like `'john.doe@example.com'` or `'198.51.100.99'`.\\n\",\n",
    "        \"5.  Use the most relevant and common fields for the specified log table and task.\\n\",\n",
    "        \"\\n\",\n",
    "        \"---\\n\",\n",
    "        \"Example:\\n\",\n",
    "        \"\\n\",\n",
    "        \"User Request: Show me failed logins for john.doe@example.com in the last day.\\n\",\n",
    "        \"\\n\",\n",
    "        \"KQL Output:\\n\",\n",
    "        \"SigninLogs | where UserPrincipalName =~ 'john.doe@example.com' and ResultType != 0 and TimeGenerated > ago(1d) | project TimeGenerated, IPAddress, Location, AppDisplayName, ResultType, ResultDescription\\n\",\n",
    "        \"---\\n\",\n",
    "        \"\\n\",\n",
    "        \"User Request: {instruction}\\n\",\n",
    "        \"\\n\",\n",
    "        \"KQL Output:\\n\",\n",
    "        \"\\\"\\\"\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"def build_training_example(rec):\\n\",\n",
    "        \"    return f\\\"{INSTRUCTION_PREFIX}{rec['instruction']}\\\\nKQL:\\\\n{rec['kql']}\\\"\\n\",\n",
    "        \"\\n\",\n",
    "        \"formatted_train=[build_training_example(r) for r in train_records]\\n\",\n",
    "        \"formatted_eval=[build_training_example(r) for r in eval_records] if eval_records else []\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"id\": \"d4e016c8\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"d4e016c8\"\n",
    "      },\n",
    "      \"outputs\": [],\n",
    "      \"source\": [\n",
    "        \"from datasets import Dataset,DatasetDict\\n\",\n",
    "        \"train_ds=Dataset.from_list([{'text':t} for t in formatted_train])\\n\",\n",
    "        \"if formatted_eval:\\n\",\n",
    "        \"    eval_ds=Dataset.from_list([{'text':t} for t in formatted_eval])\\n\",\n",
    "        \"    ds=DatasetDict({'train':train_ds,'eval':eval_ds})\\n\",\n",
    "        \"else:\\n\",\n",
    "        \"    if len(train_ds)>5:\\n\",\n",
    "        \"        split=train_ds.train_test_split(test_size=0.1,seed=42)\\n\",\n",
    "        \"        ds=DatasetDict({'train':split['train'],'eval':split['test']})\\n\",\n",
    "        \"    else:\\n\",\n",
    "        \"        ds=DatasetDict({'train':train_ds,'eval':train_ds})\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"from google.colab import userdata\\n\",\n",
    "        \"from huggingface_hub import login\\n\",\n",
    "        \"\\n\",\n",
    "        \"HF_TOKEN = userdata.get('HF_TOKEN')\\n\",\n",
    "        \"login(token=HF_TOKEN)\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"ahVVsMVj6o8M\"\n",
    "      },\n",
    "      \"id\": \"ahVVsMVj6o8M\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"id\": \"944d3a17\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"944d3a17\",\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\",\n",
    "          \"height\": 153\n",
    "        },\n",
    "        \"outputId\": \"668d6ff2-43d1-49ed-834e-f6c96843b1d1\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"...\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"from transformers import AutoTokenizer\\n\",\n",
    "        \"MAX_SEQ_LEN=1024 if 'MAX_SEQ_LEN' not in globals() else MAX_SEQ_LEN\\n\",\n",
    "        \"if 'tokenizer' not in globals():\\n\",\n",
    "        \"    last_err=None\\n\",\n",
    "        \"    for mid in CANDIDATE_MODELS:\\n\",\n",
    "        \"        try:\\n\",\n",
    "        \"            tokenizer=AutoTokenizer.from_pretrained(mid,use_fast=True,token=HF_TOKEN)\\n\",\n",
    "        \"            print('Loaded tokenizer from',mid)\\n\",\n",
    "        \"            BASE_MODEL=mid\\n\",\n",
    "        \"            break\\n\",\n",
    "        \"        except Exception as e:\\n\",\n",
    "        \"            last_err=e\\n\",\n",
    "        \"            print('Tokenizer load failed for',mid,'->',type(e).__name__,str(e)[:120])\\n\",\n",
    "        \"    if 'tokenizer' not in globals():\\n\",\n",
    "        \"        raise last_err\\n\",\n",
    "        \"if tokenizer.pad_token is None:\\n\",\n",
    "        \"    tokenizer.pad_token=tokenizer.eos_token\\n\",\n",
    "        \"print('Train examples:',len(ds['train']),' Eval examples:',len(ds['eval']))\\n\",\n",
    "        \"\\n\",\n",
    "        \"def tokenize(batch):\\n\",\n",
    "        \"    return tokenizer(batch['text'],max_length=MAX_SEQ_LEN,truncation=True)\\n\",\n",
    "        \"\\n\",\n",
    "        \"tokenized=ds.map(tokenize,batched=True,remove_columns=['text'])\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [\n",
    "        \"# !pip install -U bitsandbytes\"\n",
    "      ],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"lNWJohTRAZHA\",\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\",\n",
    "          \"height\": 690\n",
    "        },\n",
    "        \"outputId\": \"5c6a2775-c062-4a04-c009-3ab3c3a7ec75\"\n",
    "      },\n",
    "      \"id\": \"lNWJohTRAZHA\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"...\"\n",
    "          ]\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"execution_count\": null,\n",
    "      \"id\": \"b59e3473\",\n",
    "      \"metadata\": {\n",
    "        \"id\": \"b59e3473\",\n",
    "        \"colab\": {\n",
    "          \"base_uri\": \"https://localhost:8080/\",\n",
    "          \"height\": 813\n",
    "        },\n",
    "        \"outputId\": \"35ead144-38a8-415e-c921-7f84ec2ca701\"\n",
    "      },\n",
    "      \"outputs\": [\n",
    "        {\n",
    "          \"output_type\": \"stream\",\n",
    "          \"name\": \"stdout\",\n",
    "          \"text\": [\n",
    "            \"...\"\n",
    "          ]\n",
    "        }\n",
    "      ],\n",
    "      \"source\": [\n",
    "        \"from transformers import BitsAndBytesConfig,DataCollatorForLanguageModeling,TrainingArguments,Trainer,AutoModelForCausalLM,set_seed\\n\",\n",
    "        \"from peft import LoraConfig,get_peft_model,prepare_model_for_kbit_training\\n\",\n",
    "        \"import math,torch,os,shutil\\n\",\n",
    "        \"bnb_config=None\\n\",\n",
    "        \"if USE_4BIT and torch.cuda.is_available():\\n\",\n",
    "        \"    bnb_config=BitsAndBytesConfig(load_in_4bit=True,bnb_4bit_quant_type='nf4',bnb_4bit_use_double_quant=True,bnb_4bit_compute_dtype=torch.bfloat16)\\n\",\n",
    "        \"model=None\\n\",\n",
    "        \"last_err=None\\n\",\n",
    "        \"for mid in CANDIDATE_MODELS:\\n\",\n",
    "        \"    try:\\n\",\n",
    "        \"        model=AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "        \"            mid,\\n\",\n",
    "        \"            quantization_config=bnb_config,\\n\",\n",
    "        \"            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\\n\",\n",
    "        \"            device_map='auto' if (torch.cuda.is_available() and bnb_config) else None,\\n\",\n",
    "        \"            trust_remote_code=True,\\n\",\n",
    "        \"            token=HF_TOKEN\\n\",\n",
    "        \"        ) if bnb_config else AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "        \"            mid,\\n\",\n",
    "        \"            torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\\n\",\n",
    "        \"            trust_remote_code=True,\\n\",\n",
    "        \"            token=HF_TOKEN\\n\",\n",
    "        \"        )\\n\",\n",
    "        \"        print('Loaded model from',mid)\\n\",\n",
    "        \"        BASE_MODEL=mid\\n\",\n",
    "        \"        break\\n\",\n",
    "        \"    except Exception as e:\\n\",\n",
    "        \"        last_err=e\\n\",\n",
    "        \"        print('Model load failed for',mid,'->',type(e).__name__,str(e)[:160])\\n\",\n",
    "        \"if model is None:\\n\",\n",
    "        \"    raise last_err\\n\",\n",
    "        \"if bnb_config: model=prepare_model_for_kbit_training(model)\\n\",\n",
    "        \"lora_cfg=LoraConfig(r=LORA_R,lora_alpha=LORA_ALPHA,lora_dropout=LORA_DROPOUT,bias='none',task_type='CAUSAL_LM',target_modules=['q_proj','k_proj','v_proj','o_proj','dense'])\\n\",\n",
    "        \"model=get_peft_model(model,lora_cfg)\\n\",\n",
    "        \"set_seed(SEED if 'SEED' in globals() else 42)\\n\",\n",
    "        \"collator=DataCollatorForLanguageModeling(tokenizer=tokenizer,mlm=False)\\n\",\n",
    "        \"steps_per_epoch=math.ceil(len(tokenized['train'])/(BATCH_SIZE*GR_ACCUM))\\n\",\n",
    "        \"warmup_steps=2\\n\",\n",
    "        \"training_args=TrainingArguments(output_dir='train_out',per_device_train_batch_size=BATCH_SIZE,per_device_eval_batch_size=BATCH_SIZE,gradient_accumulation_steps=GR_ACCUM,learning_rate=LEARNING_RATE,warmup_steps=warmup_steps,num_train_epochs=NUM_EPOCHS,logging_steps=1,evaluation_strategy='epoch',save_strategy='epoch',bf16=torch.cuda.is_available(),gradient_checkpointing=torch.cuda.is_available(),report_to=[],optim='paged_adamw_8bit' if (bnb_config is not None) else 'adamw_torch')\\n\",\n",
    "        \"trainer=Trainer(model=model,args=training_args,train_dataset=tokenized['train'],eval_dataset=tokenized['eval'],data_collator=collator,tokenizer=tokenizer)\\n\",\n",
    "        \"trainer.train(); trainer.save_state()\\n\",\n",
    "        \"model.save_pretrained(OUTPUT_DIR); tokenizer.save_pretrained(OUTPUT_DIR)\\n\",\n",
    "        \"prompt='failed logins last 1 hour'\\n\",\n",
    "        \"inputs=tokenizer(f\\\"{INSTRUCTION_PREFIX}{prompt}\\\\nKQL:\\\\n\\\",return_tensors='pt')\\n\",\n",
    "        \"if torch.cuda.is_available(): inputs=inputs.to(model.device)\\n\",\n",
    "        \"with torch.no_grad(): out=model.generate(**inputs,max_new_tokens=120,temperature=0.2,do_sample=False,pad_token_id=tokenizer.eos_token_id)\\n\",\n",
    "        \"full=tokenizer.decode(out[0],skip_special_tokens=True)\\n\",\n",
    "        \"open('gen_sample.txt','w').write(full)\\n\",\n",
    "        \"zip_name=OUTPUT_DIR+'.zip'\\n\",\n",
    "        \"if os.path.exists(zip_name): os.remove(zip_name)\\n\",\n",
    "        \"shutil.make_archive(OUTPUT_DIR,'zip',OUTPUT_DIR)\\n\",\n",
    "        \"try:\\n\",\n",
    "        \"    from google.colab import files; files.download(zip_name)\\n\",\n",
    "        \"except Exception:\\n\",\n",
    "        \"    pass\"\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"cell_type\": \"code\",\n",
    "      \"source\": [],\n",
    "      \"metadata\": {\n",
    "        \"id\": \"SwPm6xJ6hbwQ\"\n",
    "      },\n",
    "      \"id\": \"SwPm6xJ6hbwQ\",\n",
    "      \"execution_count\": null,\n",
    "      \"outputs\": []\n",
    "    }\n",
    "  ],\n",
    "  \"metadata\": {\n",
    "    \"language_info\": {\n",
    "      \"name\": \"python\"\n",
    "    },\n",
    "    \"colab\": {\n",
    "      \"provenance\": [],\n",
    "      \"gpuType\": \"T4\"\n",
    "    },\n",
    "    \"accelerator\": \"GPU\",\n",
    "    \"kernelspec\": {\n",
    "      \"name\": \"python3\",\n",
    "      \"display_name\": \"Python 3\"\n",
    "    },\n",
    "    \"widgets\": {\n",
    "      \"application/vnd.jupyter.widget-state+json\": {\n",
    "        \"...\"\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"nbformat\": 4,\n",
    "  \"nbformat_minor\": 5\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
